{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac15cf59",
   "metadata": {},
   "source": [
    "# Pipeline ETL: Silver-to-Gold (DW)\n",
    "\n",
    "Este notebook executa o processo de ETL (Extração, Transformação e Carga) para mover dados da camada Silver (uber_silver) para a camada Gold (Data Warehouse).\n",
    "\n",
    "Lógica:\n",
    "1.  Dimensões: Carga incremental (apenas insere novos registos).\n",
    "2.  Factos: Carga completa (TRUNCATE + INSERT ... SELECT) otimizada em SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b27a7",
   "metadata": {},
   "source": [
    "## 0. Importação das Bibliotecas\n",
    "\n",
    "Importa as bibliotecas necessárias para o pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adee1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b10da9",
   "metadata": {},
   "source": [
    "## 1. Configuração do Logging\n",
    "\n",
    "Configura o sistema de logging para registar informações, erros e o progresso do script em vez de usar print()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1be856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    handlers=[\n",
    "                        logging.StreamHandler(sys.stdout) \n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae265109",
   "metadata": {},
   "source": [
    "## 2. Ligação à Base de Dados\n",
    "\n",
    "Define a função connect_to_postgres que estabelece a ligação à base de dados PostgreSQL. Esta usa variáveis de ambiente para as credenciais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28110573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_postgres():\n",
    "    try:\n",
    "        db_user = os.getenv('POSTGRES_USER', 'admin')\n",
    "        db_password = os.getenv('POSTGRES_PASSWORD', 'admin')\n",
    "        db_name = os.getenv('POSTGRES_DB', 'postgres')\n",
    "        db_host = 'localhost'\n",
    "        \n",
    "        conn_string = f\"postgresql://{db_user}:{db_password}@{db_host}/{db_name}\"\n",
    "        \n",
    "        logging.info(\"Criando engine do Postgres...\")\n",
    "        engine = create_engine(conn_string)\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            logging.info(\"Ligação com o Postgres estabelecida com sucesso!\")\n",
    "        \n",
    "        return engine\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Falha ao ligar/criar engine: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de759e",
   "metadata": {},
   "source": [
    "## 3. Configuração e Início do Pipeline\n",
    "\n",
    "Define o esquema de destino (dw) e inicializa a ligação à base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46e917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 22:00:30 - INFO - A iniciar carga incremental para o esquema 'dw'...\n",
      "2025-11-07 22:00:30 - INFO - Criando engine do Postgres...\n",
      "2025-11-07 22:00:30 - INFO - Ligação com o Postgres estabelecida com sucesso!\n"
     ]
    }
   ],
   "source": [
    "SCHEMA_NAME = 'dw'\n",
    "logging.info(f\"A iniciar carga incremental para o esquema '{SCHEMA_NAME}'...\")\n",
    "\n",
    "engine = connect_to_postgres()\n",
    "\n",
    "if engine is None:\n",
    "    logging.critical(\"Ligação à base de dados falhou. A abortar o pipeline.\")\n",
    "    raise Exception(\"Falha na ligação à Base de Dados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0541d375",
   "metadata": {},
   "source": [
    "## 4. ETAPA 1: Atualização Incremental das Dimensões\n",
    "\n",
    "Nesta etapa, atualizamos as tabelas de dimensão (dim_cus, dim_veh, dim_pay, dim_loc).\n",
    "\n",
    "A lógica é incremental: \n",
    "1.  Lemos as chaves de negócio (ex: customer_id) já existentes no DW.\n",
    "2.  Lemos as chaves de negócio da tabela uber_silver.\n",
    "3.  Identificamos quais chaves existem na silver mas não no DW.\n",
    "4.  Inserimos (append) apenas as chaves novas. A base de dados (via SERIAL) trata da geração da srk_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b50dbd",
   "metadata": {},
   "source": [
    "### 4.1. A atualizar dw.dim_cus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "919e506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 22:02:10 - INFO - Atualizando 'dim_cus'...\n",
      "2025-11-07 22:02:14 - INFO - -> Inseridos 147580 novos registos em 'dim_cus'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Atualizando 'dim_cus'...\")\n",
    "    dw_cus = pd.read_sql(f\"SELECT DISTINCT customer_id FROM {SCHEMA_NAME}.dim_cus\", engine)\n",
    "    silver_cus = pd.read_sql(\"SELECT DISTINCT customer_id FROM dw.uber_silver WHERE customer_id IS NOT NULL\", engine)\n",
    "    \n",
    "    new_cus = silver_cus[~silver_cus['customer_id'].isin(dw_cus['customer_id'])]\n",
    "    \n",
    "    if not new_cus.empty:\n",
    "        new_cus.to_sql('dim_cus', engine, schema=SCHEMA_NAME, if_exists='append', index=False)\n",
    "        logging.info(f\"-> Inseridos {len(new_cus)} novos registos em 'dim_cus'.\")\n",
    "    else:\n",
    "        logging.info(\"-> 'dim_cus' já estava atualizada.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"ERRO ao atualizar 'dim_cus': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e6da8",
   "metadata": {},
   "source": [
    "### 4.2. A atualizar dw.dim_veh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2a612ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 22:02:16 - INFO - Atualizando 'dim_veh'...\n",
      "2025-11-07 22:02:16 - INFO - -> Inseridos 7 novos registos em 'dim_veh'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Atualizando 'dim_veh'...\")\n",
    "    dw_veh = pd.read_sql(f\"SELECT DISTINCT vehicle_type::TEXT FROM {SCHEMA_NAME}.dim_veh\", engine)\n",
    "    silver_veh = pd.read_sql(\"SELECT DISTINCT vehicle_type::TEXT FROM dw.uber_silver WHERE vehicle_type IS NOT NULL\", engine)\n",
    "    \n",
    "    new_veh = silver_veh[~silver_veh['vehicle_type'].isin(dw_veh['vehicle_type'])]\n",
    "    \n",
    "    if not new_veh.empty:\n",
    "        new_veh.to_sql('dim_veh', engine, schema=SCHEMA_NAME, if_exists='append', index=False)\n",
    "        logging.info(f\"-> Inseridos {len(new_veh)} novos registos em 'dim_veh'.\")\n",
    "    else:\n",
    "        logging.info(\"-> 'dim_veh' já estava atualizada.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"ERRO ao atualizar 'dim_veh': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc200ef1",
   "metadata": {},
   "source": [
    "### 4.3. A atualizar dw.dim_pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f29d247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 22:02:20 - INFO - Atualizando 'dim_pay'...\n",
      "2025-11-07 22:02:20 - INFO - -> Inseridos 5 novos registos em 'dim_pay'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Atualizando 'dim_pay'...\")\n",
    "    dw_pay = pd.read_sql(f\"SELECT DISTINCT payment_method::TEXT FROM {SCHEMA_NAME}.dim_pay\", engine)\n",
    "    silver_pay = pd.read_sql(\"SELECT DISTINCT payment_method::TEXT FROM dw.uber_silver WHERE payment_method IS NOT NULL\", engine)\n",
    "\n",
    "    new_pay = silver_pay[~silver_pay['payment_method'].isin(dw_pay['payment_method'])]\n",
    "    \n",
    "    if not new_pay.empty:\n",
    "        new_pay.to_sql('dim_pay', engine, schema=SCHEMA_NAME, if_exists='append', index=False)\n",
    "        logging.info(f\"-> Inseridos {len(new_pay)} novos registos em 'dim_pay'.\")\n",
    "    else:\n",
    "        logging.info(\"-> 'dim_pay' já estava atualizada.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"ERRO ao atualizar 'dim_pay': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3e2e5",
   "metadata": {},
   "source": [
    "### 4.4. A atualizar dw.dim_loc\n",
    "\n",
    "Esta dimensão usa uma chave composta (pickup_location, drop_location), pelo que a lógica usa um merge do Pandas para identificar os novos pares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27cd2be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 22:02:23 - INFO - Atualizando 'dim_loc'...\n",
      "2025-11-07 22:02:24 - INFO - -> Inseridos 30556 novos registos em 'dim_loc'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Atualizando 'dim_loc'...\")\n",
    "    dw_loc = pd.read_sql(f\"SELECT pickup_location, drop_location FROM {SCHEMA_NAME}.dim_loc\", engine)\n",
    "    silver_loc = pd.read_sql(\n",
    "        \"SELECT DISTINCT pickup_location, drop_location FROM dw.uber_silver \"\n",
    "        \"WHERE pickup_location IS NOT NULL AND drop_location IS NOT NULL\", \n",
    "        engine\n",
    "    )\n",
    "    \n",
    "    new_loc = silver_loc.merge(\n",
    "        dw_loc, \n",
    "        on=['pickup_location', 'drop_location'], \n",
    "        how='left', \n",
    "        indicator=True\n",
    "    )\n",
    "    new_loc = new_loc[new_loc['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    \n",
    "    if not new_loc.empty:\n",
    "        new_loc.to_sql('dim_loc', engine, schema=SCHEMA_NAME, if_exists='append', index=False)\n",
    "        logging.info(f\"-> Inseridos {len(new_loc)} novos registos em 'dim_loc'.\")\n",
    "    else:\n",
    "        logging.info(\"-> 'dim_loc' já estava atualizada.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"ERRO ao atualizar 'dim_loc': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a6b84",
   "metadata": {},
   "source": [
    "### 4.5. A atualizar dw.dim_time\n",
    "\n",
    "Esta etapa garante que a dimensão de data esteja populada.\n",
    "\n",
    "A lógica é idempotente:\n",
    "1.  Insere a linha padrão 'N/A' se ela não existir.\n",
    "2.  Encontra o intervalo (MIN e MAX) de datas na `uber_silver`.\n",
    "3.  Usa `generate_series` do PostgreSQL para criar todas as linhas de data nesse intervalo.\n",
    "4.  Usa `ON CONFLICT (srk_date) DO NOTHING` para inserir apenas as datas que ainda não existem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcbb86e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 22:02:28 - INFO - A popular/atualizar 'dim_date' de forma idempotente...\n",
      "2025-11-07 22:02:28 - INFO - -> 'dim_date' está populada e atualizada.\n"
     ]
    }
   ],
   "source": [
    "dim_date_load_sql = f\"\"\"\n",
    "-- ETAPA A: Garantir que a linha 'N/A' (para datas nulas) existe.\n",
    "INSERT INTO {SCHEMA_NAME}.dim_date (\n",
    "    srk_date, full_date, day_name, day_of_week, day_of_month, day_of_year,\n",
    "    month_name, month_number, quarter_number, year_number, is_weekend, is_holiday\n",
    ") VALUES (\n",
    "    -1, '1900-01-01', 'N/A', 0, 0, 0, 'N/A', 0, 0, 0, FALSE, FALSE\n",
    ")\n",
    "-- Se a srk_date -1 já existir, não faz nada.\n",
    "ON CONFLICT (srk_date) DO NOTHING;\n",
    "\n",
    "-- ETAPA B: Popular o intervalo de datas com base nos dados reais\n",
    "WITH date_range AS (\n",
    "    -- Encontra o intervalo real de datas presentes na camada Silver\n",
    "    SELECT \n",
    "        MIN(date_time::date) AS min_date,\n",
    "        MAX(date_time::date) AS max_date\n",
    "    FROM dw.uber_silver\n",
    "    WHERE date_time IS NOT NULL\n",
    ")\n",
    "INSERT INTO {SCHEMA_NAME}.dim_date (\n",
    "    srk_date, full_date, day_name, day_of_week, day_of_month, day_of_year,\n",
    "    month_name, month_number, quarter_number, year_number, is_weekend, is_holiday\n",
    ")\n",
    "SELECT\n",
    "    TO_CHAR(d, 'YYYYMMDD')::INTEGER AS srk_date,\n",
    "    d AS full_date,\n",
    "    \n",
    "    TO_CHAR(d, 'FMDay') AS day_name,\n",
    "    EXTRACT(ISODOW FROM d) AS day_of_week, \n",
    "    EXTRACT(DAY FROM d) AS day_of_month,\n",
    "    EXTRACT(DOY FROM d) AS day_of_year,\n",
    "    TO_CHAR(d, 'FMMonth') AS month_name,\n",
    "    EXTRACT(MONTH FROM d) AS month_number,\n",
    "    EXTRACT(QUARTER FROM d) AS quarter_number,\n",
    "    EXTRACT(YEAR FROM d) AS year_number,\n",
    "    EXTRACT(ISODOW FROM d) IN (6, 7) AS is_weekend,\n",
    "    FALSE AS is_holiday\n",
    "FROM\n",
    "    generate_series(\n",
    "        (SELECT min_date FROM date_range),\n",
    "        (SELECT max_date FROM date_range),\n",
    "        '1 day'::interval\n",
    "    ) AS t(d)\n",
    "-- Se a srk_date (ex: 20251107) já existir, não faz nada.\n",
    "ON CONFLICT (srk_date) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    logging.info(\"A popular/atualizar 'dim_date' de forma idempotente...\")\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(dim_date_load_sql))\n",
    "    \n",
    "    logging.info(\"-> 'dim_date' está populada e atualizada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.critical(f\"ERRO CRÍTICO ao popular 'dim_date': {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659ab0f",
   "metadata": {},
   "source": [
    "## 5. ETAPA 2: Recarga da Tabela de Factos (fat_rid)\n",
    "\n",
    "Esta é a etapa principal. Executa uma única consulta SQL que corre inteiramente no PostgreSQL.\n",
    "\n",
    "1.  TRUNCATE: A tabela de factos é limpa.\n",
    "2.  INSERT ... SELECT: A consulta SQL faz o JOIN entre a uber_silver e as dimensões (que acabámos de atualizar), buscando as srk_ corretas.\n",
    "3.  COALESCE: Garante que, se um JOIN falhar, a srk_ seja preenchida com a chave padrão 'N/A' (semeadas no DW)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032388b",
   "metadata": {},
   "source": [
    "### 5.1. Definição da Query SQL da Tabela de Factos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30cde52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 22:03:41 - INFO - Query SQL da 'fat_rid' definida.\n"
     ]
    }
   ],
   "source": [
    "fact_load_sql = f\"\"\"\n",
    "-- Etapa A: Limpar a tabela de factos\n",
    "TRUNCATE TABLE {SCHEMA_NAME}.fat_rid RESTART IDENTITY;\n",
    "\n",
    "-- Etapa B: Inserir na tabela de factos\n",
    "INSERT INTO {SCHEMA_NAME}.fat_rid (\n",
    "    -- Métricas e Factos\n",
    "    date_time, booking_status, avg_vtat, avg_ctat, cancelled_by,\n",
    "    reason_for_cancelling, incomplete_rides_reason, booking_value,\n",
    "    ride_distance, driver_rating, customer_rating,\n",
    "    \n",
    "    -- Chaves Estrangeiras (SKs)\n",
    "    srk_cus, srk_veh, srk_pay, srk_loc,\n",
    "    srk_date -- A CHAVE DE DATA\n",
    ")\n",
    "WITH\n",
    "-- Pré-calcula as chaves \"N/A\" \n",
    "default_keys AS (\n",
    "    SELECT \n",
    "        (SELECT srk_cus FROM {SCHEMA_NAME}.dim_cus WHERE customer_id = 'N/A') AS srk_cus_na,\n",
    "        (SELECT srk_veh FROM {SCHEMA_NAME}.dim_veh WHERE vehicle_type = 'N/A') AS srk_veh_na,\n",
    "        (SELECT srk_pay FROM {SCHEMA_NAME}.dim_pay WHERE payment_method = 'N/A') AS srk_pay_na,\n",
    "        (SELECT srk_loc FROM {SCHEMA_NAME}.dim_loc WHERE pickup_location = 'N/A') AS srk_loc_na\n",
    ")\n",
    "SELECT\n",
    "    -- Campos diretos da Silver\n",
    "    s.date_time, \n",
    "    (s.booking_status::TEXT)::dw.booking_status_enum,\n",
    "    s.avg_vtat, \n",
    "    s.avg_ctat, \n",
    "    (s.cancelled_by::TEXT)::dw.cancelled_by_enum,\n",
    "    (s.reason_for_cancelling::TEXT)::dw.cancellation_reason_enum,\n",
    "    (s.incomplete_ride_reason::TEXT)::dw.incomplete_reason_enum,\n",
    "    s.booking_value, \n",
    "    s.ride_distance, \n",
    "    s.driver_rating, \n",
    "    s.customer_rating,\n",
    "    \n",
    "    -- Chaves (SKs) buscadas das dimensões\n",
    "    COALESCE(cus.srk_cus, def.srk_cus_na),\n",
    "    COALESCE(veh.srk_veh, def.srk_veh_na),\n",
    "    COALESCE(pay.srk_pay, def.srk_pay_na),\n",
    "    COALESCE(loc.srk_loc, def.srk_loc_na),\n",
    "    \n",
    "    -- JOIN com a dim_date\n",
    "    COALESCE(d.srk_date, -1) -- Usa -1 para datas nulas (semeado na dim_date)\n",
    "FROM\n",
    "    dw.uber_silver AS s\n",
    "CROSS JOIN\n",
    "    default_keys AS def -- Traz as chaves 'N/A' para a consulta\n",
    "LEFT JOIN\n",
    "    {SCHEMA_NAME}.dim_cus AS cus ON s.customer_id = cus.customer_id\n",
    "LEFT JOIN\n",
    "    {SCHEMA_NAME}.dim_veh AS veh ON s.vehicle_type::TEXT = veh.vehicle_type::TEXT\n",
    "LEFT JOIN \n",
    "    {SCHEMA_NAME}.dim_pay AS pay ON s.payment_method::TEXT = pay.payment_method::TEXT\n",
    "LEFT JOIN\n",
    "    {SCHEMA_NAME}.dim_loc AS loc ON s.pickup_location = loc.pickup_location\n",
    "                                AND s.drop_location = loc.drop_location\n",
    "LEFT JOIN\n",
    "    {SCHEMA_NAME}.dim_date AS d \n",
    "    ON TO_CHAR(s.date_time, 'YYYYMMDD')::INTEGER = d.srk_date;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "logging.info(\"Query SQL da 'fat_rid' definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faec0c1",
   "metadata": {},
   "source": [
    "### 5.2. Execução da Carga da Tabela de Factos\n",
    "\n",
    "Executa a query SQL de carga dentro de uma transação para garantir a atomicidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eabc907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 22:03:43 - INFO - Executando carga da 'fat_rid' (TRUNCATE + INSERT) ...\n",
      "2025-11-07 22:03:53 - INFO - -> Carga da 'fat_rid' concluída com sucesso.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Executando carga da 'fat_rid' (TRUNCATE + INSERT) ...\")\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(fact_load_sql))\n",
    "    \n",
    "    logging.info(\"-> Carga da 'fat_rid' concluída com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.critical(f\"ERRO CRÍTICO ao recarregar 'fat_rid': {e}\")\n",
    "    logging.error(\"A tabela de factos pode estar vazia ou em estado inconsistente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0a57e",
   "metadata": {},
   "source": [
    "## 6. Limpeza e Conclusão\n",
    "\n",
    "Fecha o pool de ligações do engine do SQLAlchemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2e2a77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 22:03:53 - INFO - Engine do SQLAlchemy fechado.\n",
      "2025-11-07 22:03:53 - INFO - Processo ETL (Silver-to-Gold) concluído.\n"
     ]
    }
   ],
   "source": [
    "if engine:\n",
    "    engine.dispose()\n",
    "    logging.info(\"Engine do SQLAlchemy fechado.\")\n",
    "\n",
    "logging.info(\"Processo ETL (Silver-to-Gold) concluído.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
