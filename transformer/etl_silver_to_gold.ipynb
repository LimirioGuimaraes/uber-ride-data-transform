{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac15cf59",
   "metadata": {},
   "source": [
    "# Pipeline ETL: Silver-to-Gold (DW)\n",
    "\n",
    "Este notebook executa o processo de ETL (Extração, Transformação e Carga) para mover dados da camada Silver (uber_silver) para a camada Gold (Data Warehouse).\n",
    "\n",
    "Lógica:\n",
    "1.  Dimensões: Carga incremental (apenas insere novos registos).\n",
    "2.  Factos: Carga completa (TRUNCATE + INSERT ... SELECT) otimizada em SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b27a7",
   "metadata": {},
   "source": [
    "## 0. Importação das Bibliotecas\n",
    "\n",
    "Importa as bibliotecas necessárias para o pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adee1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b10da9",
   "metadata": {},
   "source": [
    "## 1. Configuração do Logging\n",
    "\n",
    "Configura o sistema de logging para registar informações, erros e o progresso do script em vez de usar print()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1be856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    handlers=[\n",
    "                        logging.StreamHandler(sys.stdout) \n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae265109",
   "metadata": {},
   "source": [
    "## 2. Ligação à Base de Dados\n",
    "\n",
    "Define a função connect_to_postgres que estabelece a ligação à base de dados PostgreSQL. Esta usa variáveis de ambiente para as credenciais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28110573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_postgres():\n",
    "    try:\n",
    "        db_user = os.getenv('POSTGRES_USER', 'admin')\n",
    "        db_password = os.getenv('POSTGRES_PASSWORD', 'admin')\n",
    "        db_name = os.getenv('POSTGRES_DB', 'postgres')\n",
    "        db_host = 'localhost'\n",
    "        \n",
    "        conn_string = f\"postgresql://{db_user}:{db_password}@{db_host}/{db_name}\"\n",
    "        \n",
    "        logging.info(\"Criando engine do Postgres...\")\n",
    "        engine = create_engine(conn_string)\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            logging.info(\"Ligação com o Postgres estabelecida com sucesso!\")\n",
    "        \n",
    "        return engine\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Falha ao ligar/criar engine: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de759e",
   "metadata": {},
   "source": [
    "## 3. Configuração e Início do Pipeline\n",
    "\n",
    "Define o esquema de destino (dw) e inicializa a ligação à base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46e917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:20:58 - INFO - A iniciar carga incremental para o esquema 'dw'...\n",
      "2025-11-23 15:20:58 - INFO - Criando engine do Postgres...\n",
      "2025-11-23 15:20:58 - INFO - Ligação com o Postgres estabelecida com sucesso!\n"
     ]
    }
   ],
   "source": [
    "SCHEMA_NAME = 'dw'\n",
    "logging.info(f\"A iniciar carga incremental para o esquema '{SCHEMA_NAME}'...\")\n",
    "\n",
    "engine = connect_to_postgres()\n",
    "\n",
    "if engine is None:\n",
    "    logging.critical(\"Ligação à base de dados falhou. A abortar o pipeline.\")\n",
    "    raise Exception(\"Falha na ligação à Base de Dados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0541d375",
   "metadata": {},
   "source": [
    "## 4. ETAPA 1: Atualização Incremental das Dimensões\n",
    "\n",
    "Nesta etapa, atualizamos as tabelas de dimensão (dim_cus, dim_veh, dim_pay, dim_loc).\n",
    "\n",
    "A lógica é incremental: \n",
    "1.  Lemos as chaves de negócio (ex: customer_id) já existentes no DW.\n",
    "2.  Lemos as chaves de negócio da tabela uber_silver.\n",
    "3.  Identificamos quais chaves existem na silver mas não no DW.\n",
    "4.  Inserimos (append) apenas as chaves novas. A base de dados (via SERIAL) trata da geração da srk_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b50dbd",
   "metadata": {},
   "source": [
    "### 4.1. A atualizar dw.dim_cus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "919e506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:23:58 - INFO - Atualizando 'dim_cus'...\n",
      "2025-11-23 15:24:01 - INFO - -> Inseridos 147580 novos registos em 'dim_cus'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Atualizando 'dim_cus'...\")\n",
    "    dw_cus = pd.read_sql(f\"SELECT DISTINCT cus_id FROM {SCHEMA_NAME}.dim_cus\", engine)\n",
    "    silver_cus = pd.read_sql(\"SELECT DISTINCT cus_id FROM uber_silver WHERE cus_id IS NOT NULL\", engine)\n",
    "    \n",
    "    new_cus = silver_cus[~silver_cus['cus_id'].isin(dw_cus['cus_id'])]\n",
    "    \n",
    "    if not new_cus.empty:\n",
    "        new_cus.to_sql('dim_cus', engine, schema=SCHEMA_NAME, if_exists='append', index=False)\n",
    "        logging.info(f\"-> Inseridos {len(new_cus)} novos registos em 'dim_cus'.\")\n",
    "    else:\n",
    "        logging.info(\"-> 'dim_cus' já estava atualizada.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"ERRO ao atualizar 'dim_cus': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e6da8",
   "metadata": {},
   "source": [
    "### 4.2. A atualizar dw.dim_veh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2a612ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:24:52 - INFO - Atualizando 'dim_veh'...\n",
      "2025-11-23 15:24:52 - INFO - -> Inseridos 7 novos registos em 'dim_veh'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Atualizando 'dim_veh'...\")\n",
    "    dw_veh = pd.read_sql(f\"SELECT DISTINCT veh_typ::TEXT FROM {SCHEMA_NAME}.dim_veh\", engine)\n",
    "    silver_veh = pd.read_sql(\"SELECT DISTINCT veh_typ::TEXT FROM uber_silver WHERE veh_typ IS NOT NULL\", engine)\n",
    "    \n",
    "    new_veh = silver_veh[~silver_veh['veh_typ'].isin(dw_veh['veh_typ'])]\n",
    "    \n",
    "    if not new_veh.empty:\n",
    "        new_veh.to_sql('dim_veh', engine, schema=SCHEMA_NAME, if_exists='append', index=False)\n",
    "        logging.info(f\"-> Inseridos {len(new_veh)} novos registos em 'dim_veh'.\")\n",
    "    else:\n",
    "        logging.info(\"-> 'dim_veh' já estava atualizada.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"ERRO ao atualizar 'dim_veh': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc200ef1",
   "metadata": {},
   "source": [
    "### 4.3. A atualizar dw.dim_pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f29d247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:26:16 - INFO - Atualizando 'dim_pay'...\n",
      "2025-11-23 15:26:16 - INFO - -> Inseridos 5 novos registos em 'dim_pay'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Atualizando 'dim_pay'...\")\n",
    "    dw_pay = pd.read_sql(f\"SELECT DISTINCT pay_mtd::TEXT FROM {SCHEMA_NAME}.dim_pay\", engine)\n",
    "    silver_pay = pd.read_sql(\"SELECT DISTINCT pay_mtd::TEXT FROM uber_silver WHERE pay_mtd IS NOT NULL\", engine)\n",
    "\n",
    "    new_pay = silver_pay[~silver_pay['pay_mtd'].isin(dw_pay['pay_mtd'])]\n",
    "    \n",
    "    if not new_pay.empty:\n",
    "        new_pay.to_sql('dim_pay', engine, schema=SCHEMA_NAME, if_exists='append', index=False)\n",
    "        logging.info(f\"-> Inseridos {len(new_pay)} novos registos em 'dim_pay'.\")\n",
    "    else:\n",
    "        logging.info(\"-> 'dim_pay' já estava atualizada.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"ERRO ao atualizar 'dim_pay': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3e2e5",
   "metadata": {},
   "source": [
    "### 4.4. A atualizar dw.dim_loc\n",
    "\n",
    "Esta dimensão usa uma chave composta (pickup_location, drop_location), pelo que a lógica usa um merge do Pandas para identificar os novos pares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27cd2be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:28:53 - INFO - Atualizando 'dim_loc'...\n",
      "2025-11-23 15:28:54 - INFO - -> Inseridos 30556 novos registos em 'dim_loc'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Atualizando 'dim_loc'...\")\n",
    "    dw_loc = pd.read_sql(f\"SELECT pic_loc, drp_loc FROM {SCHEMA_NAME}.dim_loc\", engine)\n",
    "    silver_loc = pd.read_sql(\n",
    "        \"SELECT DISTINCT pic_loc, drp_loc FROM uber_silver \"\n",
    "        \"WHERE pic_loc IS NOT NULL AND drp_loc IS NOT NULL\", \n",
    "        engine\n",
    "    )\n",
    "    \n",
    "    new_loc = silver_loc.merge(\n",
    "        dw_loc, \n",
    "        on=['pic_loc', 'drp_loc'], \n",
    "        how='left', \n",
    "        indicator=True\n",
    "    )\n",
    "    new_loc = new_loc[new_loc['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    \n",
    "    if not new_loc.empty:\n",
    "        new_loc.to_sql('dim_loc', engine, schema=SCHEMA_NAME, if_exists='append', index=False)\n",
    "        logging.info(f\"-> Inseridos {len(new_loc)} novos registos em 'dim_loc'.\")\n",
    "    else:\n",
    "        logging.info(\"-> 'dim_loc' já estava atualizada.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"ERRO ao atualizar 'dim_loc': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a6b84",
   "metadata": {},
   "source": [
    "### 4.5. A atualizar dw.dim_time\n",
    "\n",
    "Esta etapa garante que a dimensão de data esteja populada.\n",
    "\n",
    "A lógica é idempotente:\n",
    "1.  Insere a linha padrão 'N/A' se ela não existir.\n",
    "2.  Encontra o intervalo (MIN e MAX) de datas na `uber_silver`.\n",
    "3.  Usa `generate_series` do PostgreSQL para criar todas as linhas de data nesse intervalo.\n",
    "4.  Usa `ON CONFLICT (srk_date) DO NOTHING` para inserir apenas as datas que ainda não existem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcbb86e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:34:09 - INFO - A popular/atualizar 'dim_date' de forma idempotente...\n",
      "2025-11-23 15:34:09 - INFO - -> 'dim_date' está populada e atualizada.\n"
     ]
    }
   ],
   "source": [
    "dim_date_load_sql = f\"\"\"\n",
    "INSERT INTO {SCHEMA_NAME}.dim_dat (\n",
    "    srk_dat, fll_dat, day_nme, day_wek, day_mth, day_yea,\n",
    "    mth_nme, mth_nbr, qtr_nbr, yea_nbr, is_wkd, is_hol\n",
    ") VALUES (\n",
    "    -1, '1900-01-01', 'N/A', 0, 0, 0, 'N/A', 0, 0, 0, FALSE, FALSE\n",
    ")\n",
    "ON CONFLICT (srk_dat) DO NOTHING;\n",
    "\n",
    "WITH date_range AS (\n",
    "    SELECT \n",
    "        MIN(dtt::date) AS min_date,\n",
    "        MAX(dtt::date) AS max_date\n",
    "    FROM uber_silver\n",
    "    WHERE dtt IS NOT NULL\n",
    ")\n",
    "INSERT INTO {SCHEMA_NAME}.dim_dat (\n",
    "    srk_dat, fll_dat, day_nme, day_wek, day_mth, day_yea,\n",
    "    mth_nme, mth_nbr, qtr_nbr, yea_nbr, is_wkd, is_hol\n",
    ")\n",
    "SELECT\n",
    "    TO_CHAR(d, 'YYYYMMDD')::INTEGER AS srk_dat,\n",
    "    d AS fll_dat,\n",
    "    \n",
    "    TO_CHAR(d, 'FMDay') AS day_nme,\n",
    "    EXTRACT(ISODOW FROM d) AS day_wek, \n",
    "    EXTRACT(DAY FROM d) AS day_mth,\n",
    "    EXTRACT(DOY FROM d) AS day_yea,\n",
    "    TO_CHAR(d, 'FMMonth') AS mth_nme,\n",
    "    EXTRACT(MONTH FROM d) AS mth_nbr,\n",
    "    EXTRACT(QUARTER FROM d) AS qtr_nbr,\n",
    "    EXTRACT(YEAR FROM d) AS yea_nbr,\n",
    "    EXTRACT(ISODOW FROM d) IN (6, 7) AS is_wkd,\n",
    "    FALSE AS is_hol\n",
    "FROM\n",
    "    generate_series(\n",
    "        (SELECT min_date FROM date_range),\n",
    "        (SELECT max_date FROM date_range),\n",
    "        '1 day'::interval\n",
    "    ) AS t(d)\n",
    "ON CONFLICT (srk_dat) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    logging.info(\"A popular/atualizar 'dim_date' de forma idempotente...\")\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(dim_date_load_sql))\n",
    "    \n",
    "    logging.info(\"-> 'dim_date' está populada e atualizada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.critical(f\"ERRO CRÍTICO ao popular 'dim_date': {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659ab0f",
   "metadata": {},
   "source": [
    "## 5. ETAPA 2: Recarga da Tabela de Factos (fat_rid)\n",
    "\n",
    "Esta é a etapa principal. Executa uma única consulta SQL que corre inteiramente no PostgreSQL.\n",
    "\n",
    "1.  TRUNCATE: A tabela de factos é limpa.\n",
    "2.  INSERT ... SELECT: A consulta SQL faz o JOIN entre a uber_silver e as dimensões (que acabámos de atualizar), buscando as srk_ corretas.\n",
    "3.  COALESCE: Garante que, se um JOIN falhar, a srk_ seja preenchida com a chave padrão 'N/A' (semeadas no DW)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032388b",
   "metadata": {},
   "source": [
    "### 5.1. Definição da Query SQL da Tabela de Factos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a30cde52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:39:43 - INFO - Query SQL da 'fat_rid' atualizada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "fact_load_sql = f\"\"\"\n",
    "-- Etapa A: Limpar a tabela de factos\n",
    "TRUNCATE TABLE {SCHEMA_NAME}.fat_rid RESTART IDENTITY;\n",
    "\n",
    "-- Etapa B: Inserir na tabela de factos\n",
    "INSERT INTO {SCHEMA_NAME}.fat_rid (\n",
    "    -- Métricas e Factos\n",
    "    dtt, bkg_stt, avg_vtt, avg_ctt, ccd_by,\n",
    "    rfc, irr, bkg_vle, \n",
    "    rid_dis, drv_rtg, cus_rtg,\n",
    "    \n",
    "    -- Chaves Estrangeiras (SKs)\n",
    "    srk_cus, srk_veh, srk_pay, srk_loc,\n",
    "    srk_dat\n",
    ")\n",
    "WITH\n",
    "-- Pré-calcula as chaves \"N/A\"\n",
    "default_keys AS (\n",
    "    SELECT \n",
    "        (SELECT srk_cus FROM {SCHEMA_NAME}.dim_cus WHERE cus_id = 'N/A') AS srk_cus_na,\n",
    "        (SELECT srk_veh FROM {SCHEMA_NAME}.dim_veh WHERE veh_typ = 'N/A') AS srk_veh_na,\n",
    "        (SELECT srk_pay FROM {SCHEMA_NAME}.dim_pay WHERE pay_mtd = 'N/A') AS srk_pay_na,\n",
    "        (SELECT srk_loc FROM {SCHEMA_NAME}.dim_loc WHERE pic_loc = 'N/A' AND drp_loc = 'N/A') AS srk_loc_na\n",
    ")\n",
    "SELECT\n",
    "    -- Campos diretos da tabela silver\n",
    "    s.dtt AS dtt,\n",
    "    (s.bkg_stt::TEXT)::dw.bkg_stt_enum AS bkg_stt,\n",
    "    s.avg_vtt AS avg_vtt,\n",
    "    s.avg_ctt AS avg_ctt,\n",
    "    (s.ccd_by::TEXT)::dw.ccd_by_enum AS ccd_by,\n",
    "    (s.rfc::TEXT)::dw.rfc_enum AS rfc,\n",
    "    (s.irr::TEXT)::dw.irr_enum AS irr,\n",
    "    s.bkg_vle AS bkg_vle,\n",
    "    s.rid_dis AS rid_dis,\n",
    "    s.drv_rtg AS drv_rtg,\n",
    "    s.cus_rtg AS cus_rtg,\n",
    "    \n",
    "    -- SKs\n",
    "    COALESCE(cus.srk_cus, def.srk_cus_na),\n",
    "    COALESCE(veh.srk_veh, def.srk_veh_na),\n",
    "    COALESCE(pay.srk_pay, def.srk_pay_na),\n",
    "    COALESCE(loc.srk_loc, def.srk_loc_na),\n",
    "    \n",
    "    -- SK de data\n",
    "    COALESCE(d.srk_dat, -1)\n",
    "FROM\n",
    "    uber_silver AS s\n",
    "CROSS JOIN\n",
    "    default_keys AS def\n",
    "LEFT JOIN\n",
    "    {SCHEMA_NAME}.dim_cus AS cus ON s.cus_id = cus.cus_id\n",
    "LEFT JOIN\n",
    "    {SCHEMA_NAME}.dim_veh AS veh ON s.veh_typ::TEXT = veh.veh_typ::TEXT\n",
    "LEFT JOIN\n",
    "    {SCHEMA_NAME}.dim_pay AS pay ON s.pay_mtd::TEXT = pay.pay_mtd::TEXT\n",
    "LEFT JOIN\n",
    "    {SCHEMA_NAME}.dim_loc AS loc \n",
    "    ON s.pic_loc = loc.pic_loc\n",
    "   AND s.drp_loc   = loc.drp_loc\n",
    "LEFT JOIN\n",
    "    {SCHEMA_NAME}.dim_dat AS d \n",
    "    ON TO_CHAR(s.dtt, 'YYYYMMDD')::INTEGER = d.srk_dat;\n",
    "\"\"\"\n",
    "\n",
    "logging.info(\"Query SQL da 'fat_rid' atualizada com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faec0c1",
   "metadata": {},
   "source": [
    "### 5.2. Execução da Carga da Tabela de Factos\n",
    "\n",
    "Executa a query SQL de carga dentro de uma transação para garantir a atomicidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1eabc907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:39:44 - INFO - Executando carga da 'fat_rid' (TRUNCATE + INSERT) ...\n",
      "2025-11-23 15:39:55 - INFO - -> Carga da 'fat_rid' concluída com sucesso.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Executando carga da 'fat_rid' (TRUNCATE + INSERT) ...\")\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(fact_load_sql))\n",
    "    \n",
    "    logging.info(\"-> Carga da 'fat_rid' concluída com sucesso.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.critical(f\"ERRO CRÍTICO ao recarregar 'fat_rid': {e}\")\n",
    "    logging.error(\"A tabela de factos pode estar vazia ou em estado inconsistente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0a57e",
   "metadata": {},
   "source": [
    "## 6. Limpeza e Conclusão\n",
    "\n",
    "Fecha o pool de ligações do engine do SQLAlchemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2e2a77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:39:57 - INFO - Engine do SQLAlchemy fechado.\n",
      "2025-11-23 15:39:57 - INFO - Processo ETL (Silver-to-Gold) concluído.\n"
     ]
    }
   ],
   "source": [
    "if engine:\n",
    "    engine.dispose()\n",
    "    logging.info(\"Engine do SQLAlchemy fechado.\")\n",
    "\n",
    "logging.info(\"Processo ETL (Silver-to-Gold) concluído.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16a091-16ea-4d66-8eeb-0825797f77e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
