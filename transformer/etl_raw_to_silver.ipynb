{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce8ff08",
   "metadata": {},
   "source": [
    "# ETL Pipeline raw-to-silver - Uber Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-1",
   "metadata": {},
   "source": [
    "Este bloco importa todas as bibliotecas Python necessárias para o pipeline, como pandas para manipulação de dados, SQLAlchemy para interação com o banco de dados e logging para registrar o progresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fec16ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.types import TIMESTAMP, CHAR, Enum, FLOAT, INTEGER, VARCHAR\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c63d10",
   "metadata": {},
   "source": [
    "# Etapa 0 - Criando Função de Conexão com o Banco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-2",
   "metadata": {},
   "source": [
    "Define a função `connect_to_postgres` que estabelece a conexão com o banco de dados PostgreSQL. Ela usa variáveis de ambiente para as credenciais e se conecta ao `db_host` (definido como 'localhost' neste script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b70089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_postgres():\n",
    "    db_user = os.getenv('POSTGRES_USER', 'admin')\n",
    "    db_password = os.getenv('POSTGRES_PASSWORD', 'admin')\n",
    "    db_name = os.getenv('POSTGRES_DB', 'postgres')\n",
    "    db_host = 'localhost' \n",
    "    \n",
    "    conn_string = f\"postgresql://{db_user}:{db_password}@{db_host}/{db_name}\"\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Tentando conectar ao Postgres...\")\n",
    "        engine = create_engine(conn_string)\n",
    "        connection = engine.connect()\n",
    "        logging.info(\"Conexão com o Postgres estabelecida com sucesso!\")\n",
    "        return engine, connection\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Falha ao conectar: {e}\")\n",
    "    \n",
    "    logging.critical(\"Não foi possível conectar ao banco de dados.\")\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a75f89",
   "metadata": {},
   "source": [
    "# Etapa 1 - Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-3",
   "metadata": {},
   "source": [
    "O script primeiro tenta estabelecer a conexão com o banco de dados chamando `connect_to_postgres`. Se bem-sucedido, ele lê o arquivo CSV ('uber-dataset.csv') do caminho especificado para um DataFrame pandas e registra o número de linhas lidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4687f376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:04:12,012 - INFO - Tentando conectar ao Postgres...\n",
      "2025-11-23 15:04:12,033 - INFO - Conexão com o Postgres estabelecida com sucesso!\n",
      "2025-11-23 15:04:12,035 - INFO - Iniciando Etapa 1: Extract\n",
      "2025-11-23 15:04:12,623 - INFO - 150000 linhas lidas do arquivo CSV.\n"
     ]
    }
   ],
   "source": [
    "engine, connection = connect_to_postgres()\n",
    "\n",
    "if not engine:\n",
    "    raise RuntimeError(\"Não foi possível conectar ao banco de dados. Abortando o ETL.\")\n",
    "\n",
    "logging.info(\"Iniciando Etapa 1: Extract\")\n",
    "raw_path = '../data_layer/raw/uber-dataset.csv'\n",
    "df_raw = pd.read_csv(raw_path)\n",
    "logging.info(f\"{len(df_raw)} linhas lidas do arquivo CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dba00c",
   "metadata": {},
   "source": [
    "# Etapa 2 - Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-4",
   "metadata": {},
   "source": [
    "Inicia a Etapa 2 (Transform). Este bloco executa as seguintes transformações específicas nos dados brutos:\n",
    "* Os nomes das colunas são padronizados para minúsculas e com underscores (ex: \"Booking ID\" -> \"booking_id\").\n",
    "* As colunas `date` e `time` são combinadas em uma única coluna `date_time` do tipo datetime.\n",
    "* Caracteres de aspas (`\"`) são removidos das colunas `booking_id` e `customer_id`.\n",
    "* Colunas de métricas (`avg_vtat`, `avg_ctat`, `booking_value`, `ride_distance`, `driver_ratings`, `customer_rating`) são convertidas para tipo numérico.\n",
    "* As colunas `cancelled_rides_by_customer` e `cancelled_rides_by_driver` são consolidadas na nova coluna `cancelled_by`.\n",
    "* As colunas `reason_for_cancelling_by_customer` e `driver_cancellation_reason` são consolidadas na nova coluna `reason_for_cancelling`.\n",
    "* O DataFrame final (`df_final`) é criado selecionando apenas as colunas necessárias e renomeando `driver_ratings` para `driver_rating`.\n",
    "\n",
    "**LÓGICA DE DUPLICATAS (CSV)**: Como último passo desta etapa, o script executa `df_final.drop_duplicates(subset=['booking_id'], keep='first')`. Isso remove quaisquer linhas *do próprio arquivo CSV* que tenham o mesmo `booking_id`, garantindo que apenas registros únicos avancem para a Etapa 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63516690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:04:19,403 - INFO - Iniciando Etapa 2: Transform\n",
      "2025-11-23 15:04:19,524 - INFO - Limpando aspas dos IDs...\n",
      "2025-11-23 15:04:19,797 - INFO - Removidas 1233 duplicatas encontradas no arquivo CSV (baseado no 'booking_id').\n",
      "2025-11-23 15:04:19,799 - INFO - Transformação concluída. Schema final do DataFrame:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 148767 entries, 0 to 149999\n",
      "Data columns (total 17 columns):\n",
      " #   Column   Non-Null Count   Dtype         \n",
      "---  ------   --------------   -----         \n",
      " 0   dtt      148767 non-null  datetime64[ns]\n",
      " 1   bkg_id   148767 non-null  object        \n",
      " 2   bkg_stt  148767 non-null  object        \n",
      " 3   cus_id   148767 non-null  object        \n",
      " 4   veh_typ  148767 non-null  object        \n",
      " 5   pic_loc  148767 non-null  object        \n",
      " 6   drp_loc  148767 non-null  object        \n",
      " 7   avg_vtt  148767 non-null  float64       \n",
      " 8   avg_ctt  148767 non-null  float64       \n",
      " 9   ccd_by   37191 non-null   object        \n",
      " 10  rfc      37191 non-null   object        \n",
      " 11  irr      8927 non-null    object        \n",
      " 12  bkg_vle  148767 non-null  Int64         \n",
      " 13  rid_dis  148767 non-null  float64       \n",
      " 14  drv_rtg  148767 non-null  float64       \n",
      " 15  cus_rtg  148767 non-null  float64       \n",
      " 16  pay_mtd  101175 non-null  object        \n",
      "dtypes: Int64(1), datetime64[ns](1), float64(5), object(10)\n",
      "memory usage: 20.6+ MB\n",
      "None\n",
      "                  dtt      bkg_id          bkg_stt      cus_id        veh_typ  \\\n",
      "0 2024-03-23 12:29:38  CNR5884300  No Driver Found  CID1982111          eBike   \n",
      "1 2024-11-29 18:01:39  CNR1326809       Incomplete  CID4604802       Go Sedan   \n",
      "2 2024-08-23 08:56:10  CNR8494506        Completed  CID9202816           Auto   \n",
      "3 2024-10-21 17:17:25  CNR8906825        Completed  CID2610914  Premier Sedan   \n",
      "4 2024-09-16 22:08:00  CNR1950162        Completed  CID9933542           Bike   \n",
      "\n",
      "               pic_loc            drp_loc  avg_vtt  avg_ctt ccd_by  rfc  \\\n",
      "0          Palam Vihar            Jhilmil      0.0      0.0   None  NaN   \n",
      "1        Shastri Nagar  Gurgaon Sector 56      4.9     14.0   None  NaN   \n",
      "2              Khandsa      Malviya Nagar     13.4     25.8   None  NaN   \n",
      "3  Central Secretariat           Inderlok     13.1     28.5   None  NaN   \n",
      "4     Ghitorni Village        Khan Market      5.3     19.6   None  NaN   \n",
      "\n",
      "                 irr  bkg_vle  rid_dis  drv_rtg  cus_rtg     pay_mtd  \n",
      "0                NaN        0     0.00      0.0      0.0         NaN  \n",
      "1  Vehicle Breakdown      237     5.73      0.0      0.0         UPI  \n",
      "2                NaN      627    13.58      4.9      4.9  Debit Card  \n",
      "3                NaN      416    34.02      4.6      5.0         UPI  \n",
      "4                NaN      737    48.21      4.1      4.3         UPI  \n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Iniciando Etapa 2: Transform\")\n",
    "df = df_raw.copy()\n",
    "df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "df['date_time'] = pd.to_datetime(df['date'] + ' ' + df['time'], errors='coerce')\n",
    "\n",
    "logging.info(\"Limpando aspas dos IDs...\")\n",
    "df['booking_id'] = df['booking_id'].str.strip('\"')\n",
    "df['customer_id'] = df['customer_id'].str.strip('\"')\n",
    "\n",
    "numeric_cols = ['avg_vtat', 'avg_ctat', 'booking_value', 'ride_distance', 'driver_ratings', 'customer_rating']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = (\n",
    "        pd.to_numeric(df[col].replace(\"null\", None), errors=\"coerce\")\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "conditions = [\n",
    "    df['cancelled_rides_by_customer'].notna(),\n",
    "    df['cancelled_rides_by_driver'].notna()\n",
    "]\n",
    "choices = ['customer', 'driver']\n",
    "df['cancelled_by'] = np.select(conditions, choices, default=None)\n",
    "\n",
    "df['reason_for_cancelling'] = df['reason_for_cancelling_by_customer'].fillna(df['driver_cancellation_reason'])\n",
    "\n",
    "final_columns = {\n",
    "    'date_time': 'dtt',\n",
    "    'booking_id': 'bkg_id',\n",
    "    'booking_status': 'bkg_stt',\n",
    "    'customer_id': 'cus_id',\n",
    "    'vehicle_type': 'veh_typ',\n",
    "    'pickup_location': 'pic_loc',\n",
    "    'drop_location': 'drp_loc',\n",
    "    'avg_vtat': 'avg_vtt',\n",
    "    'avg_ctat': 'avg_ctt',\n",
    "    'cancelled_by': 'ccd_by',\n",
    "    'reason_for_cancelling': 'rfc',\n",
    "    'incomplete_rides_reason': 'irr',\n",
    "    'booking_value': 'bkg_vle',\n",
    "    'ride_distance': 'rid_dis',\n",
    "    'driver_ratings': 'drv_rtg', \n",
    "    'customer_rating': 'cus_rtg',\n",
    "    'payment_method': 'pay_mtd'\n",
    "}\n",
    "\n",
    "# Seleciona apenas as colunas que vamos usar\n",
    "df_final = df[final_columns.keys()]\n",
    "\n",
    "# Renomeia as colunas para o padrão final\n",
    "df_final = df_final.rename(columns=final_columns)\n",
    "\n",
    "\n",
    "df_final['bkg_vle'] = df_final['bkg_vle'].astype('Int64')\n",
    "\n",
    "\n",
    "total_before = len(df_final)\n",
    "df_final = df_final.drop_duplicates(subset=['bkg_id'], keep='first')\n",
    "total_after = len(df_final)\n",
    "if total_before > total_after:\n",
    "    logging.info(f\"Removidas {total_before - total_after} duplicatas encontradas no arquivo CSV (baseado no 'booking_id').\")\n",
    "else:\n",
    "    logging.info(\"Nenhuma duplicata encontrada no arquivo CSV.\")\n",
    "\n",
    "\n",
    "logging.info(\"Transformação concluída. Schema final do DataFrame:\")\n",
    "print(df_final.info())\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0aa12",
   "metadata": {},
   "source": [
    "# Etapa 3 - Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-5",
   "metadata": {},
   "source": [
    "Esta etapa carrega o DataFrame `df_final` no banco de dados.\n",
    "\n",
    "* Define os tipos de dados das colunas para o banco (incluindo enums e tipos numéricos/texto).\n",
    "* Carrega o DataFrame usando `to_sql` com `if_exists='append'` e mapeamento de tipos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b10c991d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:10:09,518 - INFO - Iniciando Etapa 3: Load\n",
      "2025-11-23 15:10:09,520 - INFO - Iniciando carregamento de 148767 linhas...\n",
      "2025-11-23 15:10:25,779 - INFO - 148767 linhas carregadas com sucesso na tabela 'uber_silver'!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.info(\"Iniciando Etapa 3: Load\")\n",
    "\n",
    "try:\n",
    "    table_name = 'uber_silver'\n",
    "    logging.info(f\"Iniciando carregamento de {len(df_final)} linhas...\")\n",
    "    df_final.to_sql(\n",
    "        table_name,\n",
    "        engine,\n",
    "        if_exists='append',\n",
    "        index=False\n",
    "    )\n",
    "    logging.info(f\"{len(df_final)} linhas carregadas com sucesso na tabela '{table_name}'!\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erro ao carregar dados no banco: {e}\")\n",
    "finally:\n",
    "    if 'connection' in locals() and not connection.closed:\n",
    "        connection.close()\n",
    "        logging.info(\"Conexão com o banco de dados fechada.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
